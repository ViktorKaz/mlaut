{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from download_delgado.delgado_datasets import DownloadAndConvertDelgadoDatasets\n",
    "from mleap.data import Data\n",
    "from mleap.data import ModelsContainer\n",
    "from mleap.experiments import TestOrchestrator\n",
    "from mleap.analyze_results import AnalyseResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Dataset Delgado_data/molec-biol-protein-second has a different number of arff files\n"
     ]
    }
   ],
   "source": [
    "#load datasets as pandas DataFrame\n",
    "#each dataset needs to have metadata attached to it containing the following keys:\n",
    "# class_name: (string) name of column containing the column name in which the labels are stored\n",
    "# dataset_name: (string) name of the dataset\n",
    "delgado = DownloadAndConvertDelgadoDatasets()\n",
    "datasets, metadata = delgado.download_and_extract_datasets(verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Save the datasets in HDF5 database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data()\n",
    "data.pandas_to_db(save_loc_hdf5='delgado_datasets/', datasets=datasets, \n",
    "                  dts_metadata=metadata, save_loc_hdd='data/delgado.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stp 3: Instantiate models and Test Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_container = ModelsContainer()\n",
    "instantiated_models = models_container.instantiate_models(verbose=True, \n",
    "                                                          num_parallel_jobs=4, \n",
    "                                                          RandomForestClassifier=None, \n",
    "                                                          SVM=None, \n",
    "                                                          LogisticRegression=None)\n",
    "\n",
    "test_o = TestOrchestrator(hdf5_input_path='data/delgado.hdf5', hdf5_output_path='data/experiments.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Split datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dts_list = data.list_datasets(hdf5_loc='data/delgado.hdf5',hdf5_group='delgado_datasets/')\n",
    "split_dts_list = test_o.split_datasets(dataset_paths=dts_list, split_datasets_dir='split_datasets/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Run the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Training models on dataset: abalone. Total datasets processed: 0/3 ***\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 108 out of 108 | elapsed:    4.6s finished\n",
      "[Parallel(n_jobs=4)]: Done  12 out of  12 | elapsed:    2.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   6 out of   6 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   6 out of   6 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Training models on dataset: acute_inflammation. Total datasets processed: 1/3 ***\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 108 out of 108 | elapsed:    1.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   5 out of  12 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  12 out of  12 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done   6 out of   6 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   6 out of   6 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Training models on dataset: acute_nephritis. Total datasets processed: 2/3 ***\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 108 out of 108 | elapsed:    2.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   5 out of  12 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  12 out of  12 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done   6 out of   6 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   6 out of   6 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "test_o.run_experiments(split_dts_list[0:3], instantiated_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Analyze the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******t-test******\n",
      "                                          pair  t_statistic  p_value\n",
      "0  LogisticRegression - RandomForestClassifier         2.92     0.35\n",
      "1                     LogisticRegression - SVM         2.92     0.35\n",
      "2                 RandomForestClassifier - SVM         2.92     0.35\n",
      "******sign test******\n",
      "                                          pair   p_value\n",
      "0  LogisticRegression - RandomForestClassifier  0.248213\n",
      "1                     LogisticRegression - SVM  0.248213\n",
      "2                 RandomForestClassifier - SVM  0.248213\n",
      "******t-test bonferroni correction******\n",
      "                                          pair  t_statistic  p_value\n",
      "0  LogisticRegression - RandomForestClassifier         5.34     0.35\n",
      "1                     LogisticRegression - SVM         5.34     0.35\n",
      "2                 RandomForestClassifier - SVM         5.34     0.35\n",
      "******Wilcoxon test******\n",
      "                                          pair  statistic   p_value\n",
      "0  LogisticRegression - RandomForestClassifier        0.0  0.317311\n",
      "1                     LogisticRegression - SVM        0.0  0.317311\n",
      "2                 RandomForestClassifier - SVM        0.0  0.317311\n",
      "******Friedman test******\n",
      "   statistic   p_value\n",
      "0        2.0  0.367879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/viktor/Data/PhD/ml_experiments/py36/lib/python3.6/site-packages/scipy/stats/morestats.py:2385: UserWarning: Warning: sample size too small for normal approximation.\n",
      "  warnings.warn(\"Warning: sample size too small for normal approximation.\")\n"
     ]
    }
   ],
   "source": [
    "analyze = AnalyseResults('data/experiments.hdf5')\n",
    "t_test, t_test_df = analyze.perform_t_test()\n",
    "print('******t-test******')\n",
    "print(t_test_df)\n",
    "\n",
    "sign_test, sign_test_df = analyze.perform_sign_test()\n",
    "print('******sign test******')\n",
    "print(sign_test_df)\n",
    "\n",
    "t_test_bonferroni, t_test_bonferroni_df = analyze.perform_t_test_with_bonferroni_correction()\n",
    "print('******t-test bonferroni correction******')\n",
    "print(t_test_bonferroni_df)\n",
    "wilcoxon_test, wilcoxon_test_df = analyze.perform_wilcoxon()\n",
    "print('******Wilcoxon test******')\n",
    "print(wilcoxon_test_df)\n",
    "\n",
    "friedman_test, friedman_test_df = analyze.perform_friedman_test()\n",
    "print('******Friedman test******')\n",
    "print(friedman_test_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
