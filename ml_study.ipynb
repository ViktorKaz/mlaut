{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.delgado_datasets import DownloadAndConvertDelgadoDatasets\n",
    "from mleap.data import Data\n",
    "from mleap.data import ModelsContainer\n",
    "from mleap.experiments import TestOrchestrator\n",
    "from mleap.analyze_results import AnalyseResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Dataset Delgado_data/molec-biol-protein-second has a different number of arff files\n"
     ]
    }
   ],
   "source": [
    "delgado = DownloadAndConvertDelgadoDatasets()\n",
    "datasets, dataset_names, metadata = delgado.download_and_extract_datasets(verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data()\n",
    "data.pandas_to_db('delgado_datasets/', datasets, metadata, 'data/delgado.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate models\n",
    "models_container = ModelsContainer()\n",
    "instantiated_models = models_container.instantiate_models(verbose=True, num_parallel_jobs=4, RandomForestClassifier=None, SVM=None, LogisticRegression=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_o = TestOrchestrator(hdf5_input_path='data/delgado.hdf5', hdf5_output_path='data/experiments.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split datasets\n",
    "data = Data()\n",
    "dts_list = data.list_datasets('data/delgado.hdf5','delgado_datasets/')\n",
    "dts_list = ['delgado_datasets/' + dts for dts in dts_list]\n",
    "test_o.split_datasets(dts_list, 'split_datasets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_datasets_list = data.list_datasets('data/experiments.hdf5', 'split_datasets')\n",
    "split_datasets_list = ['split_datasets/' +dts for dts in split_datasets_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Training models on dataset: abalone. Total datasets processed: 0/3 ***\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 108 out of 108 | elapsed:    5.8s finished\n",
      "[Parallel(n_jobs=4)]: Done  12 out of  12 | elapsed:    1.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   6 out of   6 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   6 out of   6 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Training models on dataset: acute_inflammation. Total datasets processed: 1/3 ***\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 108 out of 108 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=4)]: Done   5 out of  12 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  12 out of  12 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   6 out of   6 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   6 out of   6 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Training models on dataset: acute_nephritis. Total datasets processed: 2/3 ***\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 108 out of 108 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=4)]: Done   5 out of  12 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done  12 out of  12 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   6 out of   6 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   6 out of   6 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "test_o.run_experiments(split_datasets_list[0:3], instantiated_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******t-test******\n",
      "                                          pair  t_statistic  p_value\n",
      "0  LogisticRegression - RandomForestClassifier         2.92     0.35\n",
      "1                     LogisticRegression - SVM         2.92     0.35\n",
      "2                 RandomForestClassifier - SVM         2.92     0.35\n",
      "******sign test******\n",
      "                                          pair   p_value\n",
      "0  LogisticRegression - RandomForestClassifier  0.248213\n",
      "1                     LogisticRegression - SVM  0.248213\n",
      "2                 RandomForestClassifier - SVM  0.248213\n",
      "******t-test bonferroni correction******\n",
      "                                          pair  t_statistic  p_value\n",
      "0  LogisticRegression - RandomForestClassifier         5.34     0.35\n",
      "1                     LogisticRegression - SVM         5.34     0.35\n",
      "2                 RandomForestClassifier - SVM         5.34     0.35\n",
      "******Wilcoxon test******\n",
      "                                          pair  statistic   p_value\n",
      "0  LogisticRegression - RandomForestClassifier        0.0  0.317311\n",
      "1                     LogisticRegression - SVM        0.0  0.317311\n",
      "2                 RandomForestClassifier - SVM        0.0  0.317311\n",
      "******Friedman test******\n",
      "   statistic   p_value\n",
      "0        2.0  0.367879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/viktor/Data/PhD/ml_experiments/py36/lib/python3.6/site-packages/scipy/stats/morestats.py:2385: UserWarning: Warning: sample size too small for normal approximation.\n",
      "  warnings.warn(\"Warning: sample size too small for normal approximation.\")\n"
     ]
    }
   ],
   "source": [
    "#analyze results\n",
    "analyze = AnalyseResults('data/experiments.hdf5')\n",
    "t_test, t_test_df = analyze.perform_t_test()\n",
    "print('******t-test******')\n",
    "print(t_test_df)\n",
    "\n",
    "sign_test, sign_test_df = analyze.perform_sign_test()\n",
    "print('******sign test******')\n",
    "print(sign_test_df)\n",
    "\n",
    "t_test_bonferroni, t_test_bonferroni_df = analyze.perform_t_test_with_bonferroni_correction()\n",
    "print('******t-test bonferroni correction******')\n",
    "print(t_test_bonferroni_df)\n",
    "wilcoxon_test, wilcoxon_test_df = analyze.perform_wilcoxon()\n",
    "print('******Wilcoxon test******')\n",
    "print(wilcoxon_test_df)\n",
    "\n",
    "friedman_test, friedman_test_df = analyze.perform_friedman_test()\n",
    "print('******Friedman test******')\n",
    "print(friedman_test_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
