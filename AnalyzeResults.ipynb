{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from mlaut.analyze_results import AnalyseResults\n",
    "from mlaut.data import Data\n",
    "import pandas as pd\n",
    "from mlaut.estimators.estimators import instantiate_default_estimators\n",
    "from mlaut.analyze_results.scores import ScoreAccuracy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_columns = 50\n",
    "# import Orange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data()\n",
    "input_io = data.open_hdf5('data/openml.h5', mode='r')\n",
    "out_io = data.open_hdf5('data/openml-classification.h5', mode='r')\n",
    "analyze = AnalyseResults(hdf5_output_io=out_io, \n",
    "                        hdf5_input_io=input_io, \n",
    "                        input_h5_original_datasets_group='openml/', \n",
    "                        output_h5_predictions_group='experiments/predictions/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlaut.estimators.nn_estimators import Deep_NN_Classifier\n",
    "hyperparameters = {'epochs': [50,100], \n",
    "                    'batch_size': [0, 50, 100]}\n",
    "def keras_model1(num_classes, input_dim):\n",
    "    model = OverwrittenSequentialClassifier()\n",
    "    model.add(Dense(288, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(144, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(12, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model_optimizer = optimizers.Adam(lr=0.001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=model_optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "deep_nn_4_layer_thin_dropout = Deep_NN_Classifier(keras_model=keras_model1, \n",
    "                            properties={'name':'NN-4-layer_thin_dropout'})\n",
    "\n",
    "\n",
    "def keras_model2(num_classes, input_dim):\n",
    "    nn_deep_model = OverwrittenSequentialClassifier()\n",
    "    nn_deep_model.add(Dense(2500, input_dim=input_dim, activation='relu'))\n",
    "    nn_deep_model.add(Dense(2000, activation='relu'))\n",
    "    nn_deep_model.add(Dense(1500, activation='relu'))\n",
    "    nn_deep_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model_optimizer = optimizers.Adam(lr=0.001)\n",
    "    nn_deep_model.compile(loss='mean_squared_error', optimizer=model_optimizer, metrics=['accuracy'])\n",
    "    return nn_deep_model\n",
    "\n",
    "deep_nn_4_layer_wide_no_dropout = Deep_NN_Classifier(hyperparameters=hyperparameters,\n",
    "                            keras_model=keras_model2,\n",
    "                            properties={'name':'NN-4-layer_wide_no_dropout'})\n",
    "\n",
    "\n",
    "def keras_model3(num_classes, input_dim):\n",
    "    nn_deep_model = OverwrittenSequentialClassifier()\n",
    "    nn_deep_model.add(Dense(2500, input_dim=input_dim, activation='relu'))\n",
    "    nn_deep_model.add(Dense(2000, activation='relu'))\n",
    "    nn_deep_model.add(Dropout(0.5))\n",
    "    nn_deep_model.add(Dense(1500, activation='relu'))\n",
    "    nn_deep_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model_optimizer = optimizers.Adam(lr=0.001)\n",
    "    nn_deep_model.compile(loss='mean_squared_error', optimizer=model_optimizer, metrics=['accuracy'])\n",
    "    return nn_deep_model\n",
    "\n",
    "deep_nn_4_layer_wide_with_dropout = Deep_NN_Classifier(hyperparameters=hyperparameters,\n",
    "                            keras_model=keras_model3,\n",
    "                            properties={'name':'NN-4-layer_wide_with_dropout'})\n",
    "\n",
    "\n",
    "def keras_model4(num_classes, input_dim):\n",
    "    nn_deep_model = OverwrittenSequentialClassifier()\n",
    "    nn_deep_model.add(Dense(5000, input_dim=input_dim, activation='relu'))\n",
    "    nn_deep_model.add(Dense(4500, activation='relu'))\n",
    "    nn_deep_model.add(Dense(4000, activation='relu'))\n",
    "    nn_deep_model.add(Dropout(0.5))\n",
    "\n",
    "    nn_deep_model.add(Dense(3500, activation='relu'))\n",
    "    nn_deep_model.add(Dense(3000, activation='relu'))\n",
    "    nn_deep_model.add(Dense(2500, activation='relu'))\n",
    "    nn_deep_model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "    nn_deep_model.add(Dense(2000, activation='relu'))\n",
    "    nn_deep_model.add(Dense(1500, activation='relu'))\n",
    "    nn_deep_model.add(Dense(1000, activation='relu'))\n",
    "    nn_deep_model.add(Dropout(0.5))\n",
    "\n",
    "    nn_deep_model.add(Dense(500, activation='relu'))\n",
    "    nn_deep_model.add(Dense(250, activation='relu'))\n",
    "    nn_deep_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model_optimizer = optimizers.Adam(lr=0.001)\n",
    "    nn_deep_model.compile(loss='mean_squared_error', optimizer=model_optimizer, metrics=['accuracy'])\n",
    "    return nn_deep_model\n",
    "\n",
    "deep_nn_12_layer_wide_with_dropout = Deep_NN_Classifier(hyperparameters=hyperparameters,\n",
    "                            keras_model=keras_model4,\n",
    "                            properties={'name':'NN-12-layer_wide_with_dropout'})\n",
    "\n",
    "\n",
    "\n",
    "def keras_model_1_lr01(num_classes, input_dim):\n",
    "    model = OverwrittenSequentialClassifier()\n",
    "    model.add(Dense(288, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(144, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(12, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model_optimizer = optimizers.Adam(lr=0.1)\n",
    "    model.compile(loss='mean_squared_error', optimizer=model_optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "deep_nn_4_layer_thin_dropout_lr01 = Deep_NN_Classifier(keras_model=keras_model_1_lr01, \n",
    "                            properties={'name':'NN-4-layer_thin_dropout_lr01'})\n",
    "\n",
    "def keras_model_1_lr1(num_classes, input_dim):\n",
    "    model = OverwrittenSequentialClassifier()\n",
    "    model.add(Dense(288, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(144, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(12, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model_optimizer = optimizers.Adam(lr=1)\n",
    "    model.compile(loss='mean_squared_error', optimizer=model_optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "deep_nn_4_layer_thin_dropout_lr1 = Deep_NN_Classifier(keras_model=keras_model_1_lr1, \n",
    "                            properties={'name':'NN-4-layer_thin_dropout_lr1'})\n",
    "\n",
    "\n",
    "def keras_model_2_lr01(num_classes, input_dim):\n",
    "    nn_deep_model = OverwrittenSequentialClassifier()\n",
    "    nn_deep_model.add(Dense(2500, input_dim=input_dim, activation='relu'))\n",
    "    nn_deep_model.add(Dense(2000, activation='relu'))\n",
    "    nn_deep_model.add(Dense(1500, activation='relu'))\n",
    "    nn_deep_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model_optimizer = optimizers.Adam(lr=0.1)\n",
    "    nn_deep_model.compile(loss='mean_squared_error', optimizer=model_optimizer, metrics=['accuracy'])\n",
    "    return nn_deep_model\n",
    "\n",
    "deep_nn_4_layer_wide_no_dropout_lr01 = Deep_NN_Classifier(keras_model=keras_model_2_lr01,\n",
    "                            properties={'name':'NN-4-layer_wide_no_dropout_lr01'})\n",
    "\n",
    "\n",
    "def keras_model_2_lr1(num_classes, input_dim):\n",
    "    nn_deep_model = OverwrittenSequentialClassifier()\n",
    "    nn_deep_model.add(Dense(2500, input_dim=input_dim, activation='relu'))\n",
    "    nn_deep_model.add(Dense(2000, activation='relu'))\n",
    "    nn_deep_model.add(Dense(1500, activation='relu'))\n",
    "    nn_deep_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model_optimizer = optimizers.Adam(lr=1)\n",
    "    nn_deep_model.compile(loss='mean_squared_error', optimizer=model_optimizer, metrics=['accuracy'])\n",
    "    return nn_deep_model\n",
    "\n",
    "deep_nn_4_layer_wide_no_dropout_lr1 = Deep_NN_Classifier(keras_model=keras_model_2_lr1,\n",
    "                            properties={'name':'NN-4-layer_wide_no_dropout_lr1'})\n",
    "\n",
    "\n",
    "\n",
    "def keras_model_3_lr01(num_classes, input_dim):\n",
    "    nn_deep_model = OverwrittenSequentialClassifier()\n",
    "    nn_deep_model.add(Dense(2500, input_dim=input_dim, activation='relu'))\n",
    "    nn_deep_model.add(Dense(2000, activation='relu'))\n",
    "    nn_deep_model.add(Dropout(0.5))\n",
    "    nn_deep_model.add(Dense(1500, activation='relu'))\n",
    "    nn_deep_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model_optimizer = optimizers.Adam(lr=0.1)\n",
    "    nn_deep_model.compile(loss='mean_squared_error', optimizer=model_optimizer, metrics=['accuracy'])\n",
    "    return nn_deep_model\n",
    "\n",
    "deep_nn_4_layer_wide_with_dropout_lr01 = Deep_NN_Classifier(keras_model=keras_model_3_lr01,\n",
    "                            properties={'name':'NN-4-layer_wide_with_dropout_lr01'})\n",
    "\n",
    "\n",
    "def keras_model_3_lr1(num_classes, input_dim):\n",
    "    nn_deep_model = OverwrittenSequentialClassifier()\n",
    "    nn_deep_model.add(Dense(2500, input_dim=input_dim, activation='relu'))\n",
    "    nn_deep_model.add(Dense(2000, activation='relu'))\n",
    "    nn_deep_model.add(Dropout(0.5))\n",
    "    nn_deep_model.add(Dense(1500, activation='relu'))\n",
    "    nn_deep_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model_optimizer = optimizers.Adam(lr=1)\n",
    "    nn_deep_model.compile(loss='mean_squared_error', optimizer=model_optimizer, metrics=['accuracy'])\n",
    "    return nn_deep_model\n",
    "\n",
    "deep_nn_4_layer_wide_with_dropout_lr1 = Deep_NN_Classifier(keras_model=keras_model_3_lr1,\n",
    "                            properties={'name':'NN-4-layer_wide_with_dropout_lr1'})\n",
    "\n",
    "\n",
    "\n",
    "def keras_model_4_lr01(num_classes, input_dim):\n",
    "    nn_deep_model = OverwrittenSequentialClassifier()\n",
    "    nn_deep_model.add(Dense(5000, input_dim=input_dim, activation='relu'))\n",
    "    nn_deep_model.add(Dense(4500, activation='relu'))\n",
    "    nn_deep_model.add(Dense(4000, activation='relu'))\n",
    "    nn_deep_model.add(Dropout(0.5))\n",
    "\n",
    "    nn_deep_model.add(Dense(3500, activation='relu'))\n",
    "    nn_deep_model.add(Dense(3000, activation='relu'))\n",
    "    nn_deep_model.add(Dense(2500, activation='relu'))\n",
    "    nn_deep_model.add(Dropout(0.5))\n",
    "    \n",
    "    \n",
    "    nn_deep_model.add(Dense(2000, activation='relu'))\n",
    "    nn_deep_model.add(Dense(1500, activation='relu'))\n",
    "    nn_deep_model.add(Dense(1000, activation='relu'))\n",
    "    nn_deep_model.add(Dropout(0.5))\n",
    "    \n",
    "    nn_deep_model.add(Dense(500, activation='relu'))\n",
    "    nn_deep_model.add(Dense(250, activation='relu'))\n",
    "    nn_deep_model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    model_optimizer = optimizers.Adam(lr=0.1)\n",
    "    nn_deep_model.compile(loss='mean_squared_error', optimizer=model_optimizer, metrics=['accuracy'])\n",
    "    return nn_deep_model\n",
    "\n",
    "deep_nn_12_layer_wide_with_dropout_lr01 = Deep_NN_Classifier(keras_model=keras_model_4_lr01,\n",
    "                            properties={'name':'NN-12-layer_wide_with_dropout_lr01'})\n",
    "\n",
    "def keras_model_4_lr1(num_classes, input_dim):\n",
    "    nn_deep_model = OverwrittenSequentialClassifier()\n",
    "    nn_deep_model.add(Dense(5000, input_dim=input_dim, activation='relu'))\n",
    "    nn_deep_model.add(Dense(4500, activation='relu'))\n",
    "    nn_deep_model.add(Dense(4000, activation='relu'))\n",
    "    nn_deep_model.add(Dropout(0.5))\n",
    "\n",
    "    nn_deep_model.add(Dense(3500, activation='relu'))\n",
    "    nn_deep_model.add(Dense(3000, activation='relu'))\n",
    "    nn_deep_model.add(Dense(2500, activation='relu'))\n",
    "    nn_deep_model.add(Dropout(0.5))\n",
    "    \n",
    "    \n",
    "    nn_deep_model.add(Dense(2000, activation='relu'))\n",
    "    nn_deep_model.add(Dense(1500, activation='relu'))\n",
    "    nn_deep_model.add(Dense(1000, activation='relu'))\n",
    "    nn_deep_model.add(Dropout(0.5))\n",
    "    \n",
    "    nn_deep_model.add(Dense(500, activation='relu'))\n",
    "    nn_deep_model.add(Dense(250, activation='relu'))\n",
    "    nn_deep_model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    model_optimizer = optimizers.Adam(lr=1)\n",
    "    nn_deep_model.compile(loss='mean_squared_error', optimizer=model_optimizer, metrics=['accuracy'])\n",
    "    return nn_deep_model\n",
    "\n",
    "deep_nn_12_layer_wide_with_dropout_lr1 = Deep_NN_Classifier(keras_model=keras_model_4_lr1,\n",
    "                            properties={'name':'NN-12-layer_wide_with_dropout_lr1'})\n",
    "\n",
    "def keras_model_5_lr0001(num_classes, input_dim):\n",
    "    nn_deep_model = OverwrittenSequentialClassifier()\n",
    "    nn_deep_model.add(Dense(2000, input_dim=input_dim, activation='relu'))\n",
    "    nn_deep_model.add(Dropout(0.5))\n",
    "    nn_deep_model.add(Dense(1000, activation='relu'))\n",
    "    nn_deep_model.add(Dropout(0.5))\n",
    "    nn_deep_model.add(Dense(1000, activation='relu'))\n",
    "    nn_deep_model.add(Dropout(0.5))\n",
    "    nn_deep_model.add(Dense(50, activation='relu'))\n",
    "    nn_deep_model.add(Dropout(0.5))\n",
    "\n",
    "    nn_deep_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model_optimizer = optimizers.Adam(lr=0.001)\n",
    "    nn_deep_model.compile(loss='mean_squared_error', optimizer=model_optimizer, metrics=['accuracy'])\n",
    "    return nn_deep_model\n",
    "deep_nn_4_layer_droput_each_layer_lr0001 = Deep_NN_Classifier(keras_model=keras_model_5_lr0001,\n",
    "                                        properties={'name':'NN-4-layer-droput-each-layer_lr0001'})\n",
    "\n",
    "def keras_model_5_lr01(num_classes, input_dim):\n",
    "    nn_deep_model = OverwrittenSequentialClassifier()\n",
    "    nn_deep_model.add(Dense(2000, input_dim=input_dim, activation='relu'))\n",
    "    nn_deep_model.add(Dropout(0.5))\n",
    "    nn_deep_model.add(Dense(1000, activation='relu'))\n",
    "    nn_deep_model.add(Dropout(0.5))\n",
    "    nn_deep_model.add(Dense(1000, activation='relu'))\n",
    "    nn_deep_model.add(Dropout(0.5))\n",
    "    nn_deep_model.add(Dense(50, activation='relu'))\n",
    "    nn_deep_model.add(Dropout(0.5))\n",
    "\n",
    "    nn_deep_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model_optimizer = optimizers.Adam(lr=0.1)\n",
    "    nn_deep_model.compile(loss='mean_squared_error', optimizer=model_optimizer, metrics=['accuracy'])\n",
    "    return nn_deep_model\n",
    "deep_nn_4_layer_droput_each_layer_lr01 = Deep_NN_Classifier(keras_model=keras_model_5_lr01,\n",
    "                                        properties={'name':'NN-4-layer-droput-each-layer_lr01'})\n",
    "\n",
    "def keras_model_5_lr1(num_classes, input_dim):\n",
    "    nn_deep_model = OverwrittenSequentialClassifier()\n",
    "    nn_deep_model.add(Dense(2000, input_dim=input_dim, activation='relu'))\n",
    "    nn_deep_model.add(Dropout(0.5))\n",
    "    nn_deep_model.add(Dense(1000, activation='relu'))\n",
    "    nn_deep_model.add(Dropout(0.5))\n",
    "    nn_deep_model.add(Dense(1000, activation='relu'))\n",
    "    nn_deep_model.add(Dropout(0.5))\n",
    "    nn_deep_model.add(Dense(50, activation='relu'))\n",
    "    nn_deep_model.add(Dropout(0.5))\n",
    "\n",
    "    nn_deep_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model_optimizer = optimizers.Adam(lr=1)\n",
    "    nn_deep_model.compile(loss='mean_squared_error', optimizer=model_optimizer, metrics=['accuracy'])\n",
    "    return nn_deep_model\n",
    "deep_nn_4_layer_droput_each_layer_lr1 = Deep_NN_Classifier(keras_model=keras_model_5_lr01,\n",
    "                                        properties={'name':'NN-4-layer-droput-each-layer_lr1'})\n",
    "\n",
    "def keras_model_6_lr001(num_classes, input_dim):\n",
    "    nn_deep_model = OverwrittenSequentialClassifier()\n",
    "    nn_deep_model.add(Dropout(0.7, input_shape=(input_dim,)))\n",
    "    nn_deep_model.add(Dense(1024, activation='relu'))\n",
    "    nn_deep_model.add(Dropout(0.5))\n",
    "    nn_deep_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model_optimizer = optimizers.Adam(lr=0.001)\n",
    "    nn_deep_model.compile(loss='mean_squared_error', optimizer=model_optimizer, metrics=['accuracy'])\n",
    "    return nn_deep_model\n",
    "deep_nn_2_layer_droput_input_layer_lr001 = Deep_NN_Classifier(keras_model=keras_model_6_lr001,\n",
    "                                        properties={'name':'NN-2-layer-droput-input-layer_lr001'})\n",
    "\n",
    "def keras_model_6_lr01(num_classes, input_dim):\n",
    "    nn_deep_model = OverwrittenSequentialClassifier()\n",
    "    nn_deep_model.add(Dropout(0.7, input_shape=(input_dim,)))\n",
    "    nn_deep_model.add(Dense(1024, activation='relu'))\n",
    "    nn_deep_model.add(Dropout(0.5))\n",
    "    nn_deep_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model_optimizer = optimizers.Adam(lr=0.1)\n",
    "    nn_deep_model.compile(loss='mean_squared_error', optimizer=model_optimizer, metrics=['accuracy'])\n",
    "    return nn_deep_model\n",
    "deep_nn_2_layer_droput_input_layer_lr01 = Deep_NN_Classifier(keras_model=keras_model_6_lr01,\n",
    "                                        properties={'name':'NN-2-layer-droput-input-layer_lr01'})\n",
    "\n",
    "def keras_model_6_lr1(num_classes, input_dim):\n",
    "    nn_deep_model = OverwrittenSequentialClassifier()\n",
    "    nn_deep_model.add(Dropout(0.7, input_shape=(input_dim,)))\n",
    "    nn_deep_model.add(Dense(1024, activation='relu'))\n",
    "    nn_deep_model.add(Dropout(0.5))\n",
    "    nn_deep_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model_optimizer = optimizers.Adam(lr=1)\n",
    "    nn_deep_model.compile(loss='mean_squared_error', optimizer=model_optimizer, metrics=['accuracy'])\n",
    "    return nn_deep_model\n",
    "\n",
    "deep_nn_2_layer_droput_input_layer_lr1 = Deep_NN_Classifier(keras_model=keras_model_6_lr1,\n",
    "                                        properties={'name':'NN-2-layer-droput-input-layer_lr1'})\n",
    "\n",
    "estimators = [deep_nn_4_layer_thin_dropout_lr01,\n",
    "            deep_nn_4_layer_thin_dropout_lr1, \n",
    "            deep_nn_4_layer_wide_no_dropout_lr01,\n",
    "            deep_nn_4_layer_wide_no_dropout_lr1,\n",
    "            deep_nn_4_layer_wide_with_dropout_lr01,\n",
    "            deep_nn_4_layer_wide_with_dropout_lr1,\n",
    "            deep_nn_12_layer_wide_with_dropout_lr01,\n",
    "            deep_nn_12_layer_wide_with_dropout_lr1,\n",
    "            deep_nn_4_layer_droput_each_layer_lr0001,\n",
    "            deep_nn_4_layer_droput_each_layer_lr01,\n",
    "            deep_nn_4_layer_droput_each_layer_lr1,\n",
    "            deep_nn_4_layer_thin_dropout,\n",
    "            deep_nn_4_layer_wide_no_dropout, \n",
    "            deep_nn_4_layer_wide_with_dropout,\n",
    "            deep_nn_12_layer_wide_with_dropout,            \n",
    "              deep_nn_2_layer_droput_input_layer_lr001,\n",
    "            deep_nn_2_layer_droput_input_layer_lr01,\n",
    "            deep_nn_2_layer_droput_input_layer_lr1]\n",
    "\n",
    "estim = instantiate_default_estimators(['Classification'])\n",
    "# estimators = []\n",
    "for e in estim:\n",
    "    if e.properties['name'] is not 'NeuralNetworkDeepClassifier':\n",
    "        estimators.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (file is already open for write (may use <h5clear file> to clear file consistency flags))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-9947369d1a67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m (errors_per_estimator, \n\u001b[0;32m      8\u001b[0m  \u001b[0merrors_per_dataset_per_estimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m  errors_per_dataset_per_estimator_df) = analyze.prediction_errors(score_accuracy, estim)\n\u001b[0m",
      "\u001b[1;32mL:\\PhD\\mleap\\mlaut\\analyze_results\\analyze_results.py\u001b[0m in \u001b[0;36mprediction_errors\u001b[1;34m(self, metric, estimators, exact_match)\u001b[0m\n\u001b[0;32m     84\u001b[0m         \"\"\"\n\u001b[0;32m     85\u001b[0m         \u001b[1;31m#load all predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[0mdts_predictions_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdts_predictions_list_full_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_datasets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_h5_predictions_group\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_io\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLosses\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimators\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexact_match\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[0mdts_processed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mL:\\PhD\\mleap\\mlaut\\data\\data.py\u001b[0m in \u001b[0;36mlist_datasets\u001b[1;34m(self, hdf5_group, hdf5_io)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtuple\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0mnames\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mfull\u001b[0m \u001b[0mpath\u001b[0m \u001b[0mto\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \"\"\"\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0mdts_names_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhdf5_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_datasets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhdf5_group\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[0mdts_names_list_full_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mhdf5_group\u001b[0m  \u001b[1;33m+\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mdts\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdts\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdts_names_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdts_names_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdts_names_list_full_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mL:\\PhD\\mleap\\mlaut\\shared\\files_io.py\u001b[0m in \u001b[0;36mlist_datasets\u001b[1;34m(self, hdf5_group)\u001b[0m\n\u001b[0;32m    287\u001b[0m         \"\"\"\n\u001b[0;32m    288\u001b[0m         \u001b[0mdatasets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hdf5_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhdf5_group\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Group {hdf5_group} does not exist in {self._hdf5_filename,}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[0;32m    267\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m                 \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[0mflags\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r+'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to open file (file is already open for write (may use <h5clear file> to clear file consistency flags))"
     ]
    }
   ],
   "source": [
    "# estimators = instantiate_default_estimators(['Classification'])\n",
    "score_accuracy = ScoreAccuracy()\n",
    "\n",
    "# (errors_per_estimator, \n",
    "#  errors_per_dataset_per_estimator) = analyze.prediction_errors(metric=score_accuracy, estimators=estimators)\n",
    " \n",
    "(errors_per_estimator, \n",
    " errors_per_dataset_per_estimator, \n",
    " errors_per_dataset_per_estimator_df) = analyze.b(score_accuracy, estim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,6]\n",
    "b = [3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigger = [i[0] > i[1] for i in zip(a,b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.sum(bigger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7539062500000002"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binom_test(4,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple average and standard error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "avg_and_std_error = analyze.average_and_std_error(errors_per_estimator)\n",
    "# avg_and_std_error.index.name='Estimator Name'\n",
    "avg_and_std_error.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = analyze.plot_boxcharts(errors_per_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "avg_rank = analyze.ranks(errors_per_estimator, ascending=False)\n",
    "avg_rank.round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks= avg_rank['avg_rank'].tolist()\n",
    "names = avg_rank.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = []\n",
    "rs = []\n",
    "first_nn_found = False\n",
    "\n",
    "for r,n in zip(ranks,names):\n",
    "    if n.startswith('NN') and first_nn_found == False:\n",
    "        ns.append(n)\n",
    "        rs.append(r)\n",
    "        first_nn_found = True\n",
    "    else:\n",
    "        if not n.startswith('NN'):\n",
    "            ns.append(n)\n",
    "            rs.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cd = Orange.evaluation.compute_CD(rs,121)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Orange.evaluation.graph_ranks(avranks=rs, names=ns, cd=cd)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_training_time, training_time_per_dataset = analyze.average_training_time(estimators)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_training_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### merge avg score, rank and training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_metrics = pd.DataFrame.merge(avg_rank,avg_and_std_error, left_index=True, right_index=True)\n",
    "avg_metrics = pd.DataFrame.merge(avg_metrics, avg_training_time,left_index=True, right_index=True)\n",
    "avg_metrics\n",
    "#change names of estimators\n",
    "as_list = avg_metrics.index.tolist()\n",
    "idx = as_list.index('NN-12-layer_wide_with_dropout')\n",
    "as_list[idx] = 'NN-12-layer_wide_with_dropout_lr001'\n",
    "\n",
    "idx = as_list.index('NN-4-layer_wide_with_dropout')\n",
    "as_list[idx] = 'NN-4-layer_wide_with_dropout_lr001'\n",
    "\n",
    "idx = as_list.index('NN-4-layer_wide_no_dropout')\n",
    "as_list[idx] = 'NN-4-layer_wide_no_dropout_lr001'\n",
    "\n",
    "\n",
    "idx = as_list.index('NN-4-layer_thin_dropout')\n",
    "as_list[idx] = 'NN-4-layer_thin_dropout_lr001'\n",
    "\n",
    "avg_metrics.index = as_list\n",
    "\n",
    "avg_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cohen's d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cohens_d = analyze.cohens_d(errors_per_estimator)\n",
    "cohens_d.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t_test, t_test_df = analyze.t_test(errors_per_estimator)\n",
    "t_test_df.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_df.iloc[:,0:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sign test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_test, sign_test_df = analyze.sign_test(errors_per_estimator)\n",
    "sign_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-test with Bonferroni correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t_test_bonferroni_df = analyze.t_test_with_bonferroni_correction(errors_per_estimator)\n",
    "t_test_bonferroni_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in estim:\n",
    "    print(e.properties['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wilcoxon test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a, wilcoxon_df_multiindex = analyze.wilcoxon_test(errors_per_estimator)\n",
    "wilcoxon_df_multiindex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Friedman test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_, friedman_test_df = analyze.friedman_test(errors_per_estimator)\n",
    "friedman_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nemeniy_test = analyze.nemenyi(errors_per_estimator)\n",
    "nemeniy_test_df = pd.DataFrame(nemeniy_test)\n",
    "nemeniy_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "errors_per_dataset_per_estimator_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save tables to $\\LaTeX$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t-test\n",
    "with open('../mlaut-paper/mlaut/tables/t_test.tex', 'w') as tf:\n",
    "    tf.write(t_test_df.to_latex())\n",
    "    \n",
    "t_test1 = t_test_df.iloc[:,0:8]\n",
    "t_test2 = t_test_df.iloc[:,8:16]\n",
    "t_test3 = t_test_df.iloc[:,16:24]\n",
    "t_test4 = t_test_df.iloc[:,24:32]\n",
    "t_test5 = t_test_df.iloc[:,32:40]\n",
    "t_test6 = t_test_df.iloc[:,40:48]\n",
    "t_test7 = t_test_df.iloc[:,48:54]\n",
    "\n",
    "with open('../mlaut-paper/mlaut/tables/t_test1.tex', 'w') as tf:\n",
    "    tf.write(t_test1.to_latex())\n",
    "with open('../mlaut-paper/mlaut/tables/t_test2.tex', 'w') as tf:\n",
    "    tf.write(t_test2.to_latex()) \n",
    "with open('../mlaut-paper/mlaut/tables/t_test3.tex', 'w') as tf:\n",
    "    tf.write(t_test3.to_latex()) \n",
    "with open('../mlaut-paper/mlaut/tables/t_test4.tex', 'w') as tf:\n",
    "    tf.write(t_test4.to_latex()) \n",
    "with open('../mlaut-paper/mlaut/tables/t_test5.tex', 'w') as tf:\n",
    "    tf.write(t_test5.to_latex()) \n",
    "with open('../mlaut-paper/mlaut/tables/t_test6.tex', 'w') as tf:\n",
    "    tf.write(t_test6.to_latex()) \n",
    "with open('../mlaut-paper/mlaut/tables/t_test7.tex', 'w') as tf:\n",
    "    tf.write(t_test7.to_latex()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t-test with Bonferroni\n",
    "with open('../mlaut-paper/mlaut/tables/t_test_bonferroni.tex', 'w') as tf:\n",
    "    tf.write(t_test_bonferroni_df.to_latex())\n",
    "    \n",
    "t_test_bonferroni1 = t_test_bonferroni_df.iloc[:,0:4]\n",
    "t_test_bonferroni2 = t_test_bonferroni_df.iloc[:,4:8]\n",
    "t_test_bonferroni3 = t_test_bonferroni_df.iloc[:,8:12]\n",
    "t_test_bonferroni4 = t_test_bonferroni_df.iloc[:,12:16]\n",
    "t_test_bonferroni5 = t_test_bonferroni_df.iloc[:,16:20]\n",
    "t_test_bonferroni6 = t_test_bonferroni_df.iloc[:,20:24]\n",
    "t_test_bonferroni7 = t_test_bonferroni_df.iloc[:,24:27]\n",
    "\n",
    "with open('../mlaut-paper/mlaut/tables/t_test_bonferroni1.tex', 'w') as tf:\n",
    "    tf.write(t_test_bonferroni1.to_latex())\n",
    "with open('../mlaut-paper/mlaut/tables/t_test_bonferroni2.tex', 'w') as tf:\n",
    "    tf.write(t_test_bonferroni2.to_latex())\n",
    "with open('../mlaut-paper/mlaut/tables/t_test_bonferroni3.tex', 'w') as tf:\n",
    "    tf.write(t_test_bonferroni3.to_latex())\n",
    "with open('../mlaut-paper/mlaut/tables/t_test_bonferroni4.tex', 'w') as tf:\n",
    "    tf.write(t_test_bonferroni4.to_latex())\n",
    "with open('../mlaut-paper/mlaut/tables/t_test_bonferroni5.tex', 'w') as tf:\n",
    "    tf.write(t_test_bonferroni5.to_latex())\n",
    "with open('../mlaut-paper/mlaut/tables/t_test_bonferroni6.tex', 'w') as tf:\n",
    "    tf.write(t_test_bonferroni6.to_latex())\n",
    "with open('../mlaut-paper/mlaut/tables/t_test_bonferroni7.tex', 'w') as tf:\n",
    "    tf.write(t_test_bonferroni7.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sign test\n",
    "with open('../mlaut-paper/mlaut/tables/sign_test.tex', 'w') as tf:\n",
    "    tf.write(sign_test_df.to_latex())\n",
    "sign_test1 = sign_test_df.iloc[:,0:8]\n",
    "sign_test2 = sign_test_df.iloc[:,8:16]\n",
    "sign_test3= sign_test_df.iloc[:,16:24]\n",
    "sign_test4 = sign_test_df.iloc[:,24:32]\n",
    "sign_test5 = sign_test_df.iloc[:,32:40]\n",
    "sign_test6 = sign_test_df.iloc[:,40:48]\n",
    "sign_test7 = sign_test_df.iloc[:,48:54]\n",
    "with open('../mlaut-paper/mlaut/tables/sign_test1.tex', 'w') as tf:\n",
    "    tf.write(sign_test1.to_latex())\n",
    "with open('../mlaut-paper/mlaut/tables/sign_test2.tex', 'w') as tf:\n",
    "    tf.write(sign_test2.to_latex())\n",
    "with open('../mlaut-paper/mlaut/tables/sign_test3.tex', 'w') as tf:\n",
    "    tf.write(sign_test3.to_latex())\n",
    "with open('../mlaut-paper/mlaut/tables/sign_test4.tex', 'w') as tf:\n",
    "    tf.write(sign_test4.to_latex())\n",
    "with open('../mlaut-paper/mlaut/tables/sign_test5.tex', 'w') as tf:\n",
    "    tf.write(sign_test5.to_latex())\n",
    "with open('../mlaut-paper/mlaut/tables/sign_test6.tex', 'w') as tf:\n",
    "    tf.write(sign_test6.to_latex())\n",
    "with open('../mlaut-paper/mlaut/tables/sign_test7.tex', 'w') as tf:\n",
    "    tf.write(sign_test7.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wilcoxon\n",
    "with open('../mlaut-paper/mlaut/tables/wilxocon_test.tex', 'w') as tf:\n",
    "    tf.write(wilcoxon_df_multiindex.to_latex())\n",
    "    \n",
    "wilcoxon1 = wilcoxon_df_multiindex.iloc[:,0:8]\n",
    "wilcoxon2 = wilcoxon_df_multiindex.iloc[:,8:16]\n",
    "wilcoxon3 = wilcoxon_df_multiindex.iloc[:,16:24]\n",
    "wilcoxon4 = wilcoxon_df_multiindex.iloc[:,24:32]\n",
    "wilcoxon5 = wilcoxon_df_multiindex.iloc[:,32:40]\n",
    "wilcoxon6 = wilcoxon_df_multiindex.iloc[:,40:48]\n",
    "wilcoxon7 = wilcoxon_df_multiindex.iloc[:,48:54]\n",
    "\n",
    "with open('../mlaut-paper/mlaut/tables/wilcoxon_test1.tex', 'w') as tf:\n",
    "    tf.write(wilcoxon1.to_latex())\n",
    "with open('../mlaut-paper/mlaut/tables/wilcoxon_test2.tex', 'w') as tf:\n",
    "    tf.write(wilcoxon2.to_latex())\n",
    "with open('../mlaut-paper/mlaut/tables/wilcoxon_test3.tex', 'w') as tf:\n",
    "    tf.write(wilcoxon3.to_latex())\n",
    "with open('../mlaut-paper/mlaut/tables/wilcoxon_test4.tex', 'w') as tf:\n",
    "    tf.write(wilcoxon4.to_latex())\n",
    "with open('../mlaut-paper/mlaut/tables/wilcoxon_test5.tex', 'w') as tf:\n",
    "    tf.write(wilcoxon5.to_latex())\n",
    "with open('../mlaut-paper/mlaut/tables/wilcoxon_test6.tex', 'w') as tf:\n",
    "    tf.write(wilcoxon6.to_latex())\n",
    "with open('../mlaut-paper/mlaut/tables/wilcoxon_test7.tex', 'w') as tf:\n",
    "    tf.write(wilcoxon7.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nemeniy test\n",
    "with open('../mlaut-paper/mlaut/tables/nemeniy_test.tex', 'w') as tf:\n",
    "    tf.write(nemeniy_test_df.to_latex())\n",
    "    \n",
    "nemeniy_test1 = nemeniy_test_df.iloc[:,0:4]\n",
    "nemeniy_test2 = nemeniy_test_df.iloc[:,4:8]\n",
    "nemeniy_test3 = nemeniy_test_df.iloc[:,8:12]\n",
    "nemeniy_test4 = nemeniy_test_df.iloc[:,12:16]\n",
    "nemeniy_test5 = nemeniy_test_df.iloc[:,16:20]\n",
    "nemeniy_test6 = nemeniy_test_df.iloc[:,20:24]\n",
    "nemeniy_test7 = nemeniy_test_df.iloc[:,24:27]\n",
    "\n",
    "with open('../mlaut-paper/mlaut/tables/nemeniy_test1.tex', 'w') as tf:\n",
    "    tf.write(nemeniy_test1.to_latex())\n",
    "with open('../mlaut-paper/mlaut/tables/nemeniy_test2.tex', 'w') as tf:\n",
    "    tf.write(nemeniy_test2.to_latex())\n",
    "with open('../mlaut-paper/mlaut/tables/nemeniy_test3.tex', 'w') as tf:\n",
    "    tf.write(nemeniy_test3.to_latex())\n",
    "with open('../mlaut-paper/mlaut/tables/nemeniy_test4.tex', 'w') as tf:\n",
    "    tf.write(nemeniy_test4.to_latex())\n",
    "with open('../mlaut-paper/mlaut/tables/nemeniy_test5.tex', 'w') as tf:\n",
    "    tf.write(nemeniy_test5.to_latex())\n",
    "with open('../mlaut-paper/mlaut/tables/nemeniy_test6.tex', 'w') as tf:\n",
    "    tf.write(nemeniy_test6.to_latex())\n",
    "with open('../mlaut-paper/mlaut/tables/nemeniy_test7.tex', 'w') as tf:\n",
    "    tf.write(nemeniy_test7.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average and standard error\n",
    "with open('../mlaut-paper/mlaut/tables/avg_and_st_error.tex', 'w') as tf:\n",
    "    tf.write(avg_and_std_error.to_latex())\n",
    "#average trining time\n",
    "with open('../mlaut-paper/mlaut/tables/avg_training_time.tex', 'w') as tf:\n",
    "    tf.write(avg_training_time.to_latex())\n",
    "#average rank\n",
    "with open('../mlaut-paper/mlaut/tables/avg_rank.tex', 'w') as tf:\n",
    "    tf.write(avg_rank.to_latex())\n",
    "\n",
    "#average metrics\n",
    "with open('../mlaut-paper/mlaut/tables/avg_metrics.tex', 'w') as tf:\n",
    "    tf.write(avg_metrics.to_latex())\n",
    "#Cohen's D\n",
    "with open('../mlaut-paper/mlaut/tables/cohens_d.tex', 'w') as tf:\n",
    "    tf.write(cohens_d.to_latex())\n",
    "\n",
    "\n",
    "\n",
    "#Errors per dataset per estimator\n",
    "with open('../mlaut-paper/mlaut/tables/errors_per_dataset_per_estimator.tex', 'w') as tf:\n",
    "    tf.write(errors_per_dataset_per_estimator_df.to_latex(longtable=True))\n",
    "#              replace('\\n', '\\n\\\\caption{Errors per dataset and estimator}\\\\\\\\\\n', 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../mlaut-paper/mlaut/tables/friedman_test.tex', 'w') as tf:\n",
    "    tf.write(friedman_test_df.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save bax chart with results\n",
    "fig.savefig('../mlaut-paper/mlaut/images/boxchart_results.png', dpi=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
