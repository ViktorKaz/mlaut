{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from download_delgado.delgado_datasets import DownloadAndConvertDelgadoDatasets\n",
    "from mlaut.data import Data\n",
    "from mlaut.estimators.estimators import instantiate_default_estimators\n",
    "from mlaut.experiments import Orchestrator\n",
    "from mlaut.analyze_results import AnalyseResults\n",
    "from download_delgado.delgado_datasets import DownloadAndConvertDelgadoDatasets\n",
    "from mlaut.analyze_results.scores import ScoreAccuracy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlaut.estimators.generic_estimator import Generic_Estimator\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlaut.estimators.nn_estimators import Deep_NN_Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data()\n",
    "input_io = data.open_hdf5('data/delgado.h5', mode='r')\n",
    "out_io = data.open_hdf5('data/delgado-classification-deep.h5', mode='r')\n",
    "\n",
    "analyze = AnalyseResults(hdf5_output_io=out_io, \n",
    "                        hdf5_input_io=input_io, \n",
    "                        input_h5_original_datasets_group='openml/', \n",
    "                        output_h5_predictions_group='experiments/predictions/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlaut.estimators.nn_estimators import Deep_NN_Classifier\n",
    "hyperparameters = {'epochs': [50,100], \n",
    "                    'batch_size': [0, 50, 100]}\n",
    "def keras_model1(num_classes, input_dim):\n",
    "    model = OverwrittenSequentialClassifier()\n",
    "    model.add(Dense(288, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(144, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(12, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model_optimizer = optimizers.Adam(lr=0.001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=model_optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "deep_nn_4_layer_thin_dropout = Deep_NN_Classifier(keras_model=keras_model1, \n",
    "                            properties={'name':'NN-4-layer_thin_dropout'})\n",
    "\n",
    "\n",
    "def keras_model2(num_classes, input_dim):\n",
    "    nn_deep_model = OverwrittenSequentialClassifier()\n",
    "    nn_deep_model.add(Dense(2500, input_dim=input_dim, activation='relu'))\n",
    "    nn_deep_model.add(Dense(2000, activation='relu'))\n",
    "    nn_deep_model.add(Dense(1500, activation='relu'))\n",
    "    nn_deep_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model_optimizer = optimizers.Adam(lr=0.001)\n",
    "    nn_deep_model.compile(loss='mean_squared_error', optimizer=model_optimizer, metrics=['accuracy'])\n",
    "    return nn_deep_model\n",
    "\n",
    "deep_nn_4_layer_wide_no_dropout = Deep_NN_Classifier(hyperparameters=hyperparameters,\n",
    "                            keras_model=keras_model2,\n",
    "                            properties={'name':'NN-4-layer_wide_no_dropout'})\n",
    "\n",
    "\n",
    "def keras_model3(num_classes, input_dim):\n",
    "    nn_deep_model = OverwrittenSequentialClassifier()\n",
    "    nn_deep_model.add(Dense(2500, input_dim=input_dim, activation='relu'))\n",
    "    nn_deep_model.add(Dense(2000, activation='relu'))\n",
    "    nn_deep_model.add(Dropout(0.5))\n",
    "    nn_deep_model.add(Dense(1500, activation='relu'))\n",
    "    nn_deep_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model_optimizer = optimizers.Adam(lr=0.001)\n",
    "    nn_deep_model.compile(loss='mean_squared_error', optimizer=model_optimizer, metrics=['accuracy'])\n",
    "    return nn_deep_model\n",
    "\n",
    "deep_nn_4_layer_wide_with_dropout = Deep_NN_Classifier(hyperparameters=hyperparameters,\n",
    "                            keras_model=keras_model3,\n",
    "                            properties={'name':'NN-4-layer_wide_with_dropout'})\n",
    "\n",
    "\n",
    "def keras_model4(num_classes, input_dim):\n",
    "    nn_deep_model = OverwrittenSequentialClassifier()\n",
    "    nn_deep_model.add(Dense(5000, input_dim=input_dim, activation='relu'))\n",
    "    nn_deep_model.add(Dense(4500, activation='relu'))\n",
    "    nn_deep_model.add(Dense(4000, activation='relu'))\n",
    "    nn_deep_model.add(Dropout(0.5))\n",
    "\n",
    "    nn_deep_model.add(Dense(3500, activation='relu'))\n",
    "    nn_deep_model.add(Dense(3000, activation='relu'))\n",
    "    nn_deep_model.add(Dense(2500, activation='relu'))\n",
    "    nn_deep_model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "    nn_deep_model.add(Dense(2000, activation='relu'))\n",
    "    nn_deep_model.add(Dense(1500, activation='relu'))\n",
    "    nn_deep_model.add(Dense(1000, activation='relu'))\n",
    "    nn_deep_model.add(Dropout(0.5))\n",
    "\n",
    "    nn_deep_model.add(Dense(500, activation='relu'))\n",
    "    nn_deep_model.add(Dense(250, activation='relu'))\n",
    "    nn_deep_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model_optimizer = optimizers.Adam(lr=0.001)\n",
    "    nn_deep_model.compile(loss='mean_squared_error', optimizer=model_optimizer, metrics=['accuracy'])\n",
    "    return nn_deep_model\n",
    "\n",
    "deep_nn_12_layer_wide_with_dropout = Deep_NN_Classifier(hyperparameters=hyperparameters,\n",
    "                            keras_model=keras_model4,\n",
    "                            properties={'name':'NN-12-layer_wide_with_dropout'})\n",
    "\n",
    "\n",
    "estimators = [deep_nn_4_layer_thin_dropout,\n",
    "            deep_nn_4_layer_wide_no_dropout, \n",
    "            deep_nn_4_layer_wide_with_dropout,\n",
    "            deep_nn_12_layer_wide_with_dropout]\n",
    "\n",
    "estim = instantiate_default_estimators(['Classification'])\n",
    "# estimators = []\n",
    "for e in estim:\n",
    "    if e.properties['name'] is not 'NeuralNetworkDeepClassifier':\n",
    "        estimators.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_accuracy = ScoreAccuracy()\n",
    "\n",
    "(errors_per_estimator, \n",
    " errors_per_dataset_per_estimator, \n",
    " errors_per_dataset_per_estimator_df) = analyze.prediction_errors(score_accuracy, estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, dts_run_times_full_path = data.list_datasets('run_times', out_io)\n",
    "estimator_dict = {estimator.properties['name']: [] for estimator in estimators}\n",
    "\n",
    "for dts in dts_run_times_full_path:\n",
    "    run_times_per_estimator,_ = out_io.load_dataset_pd(dataset_path=dts, return_metadata=False)\n",
    "    run_times_estimator_names = run_times_per_estimator['strategy_name'].tolist()\n",
    "    \n",
    "    for strat in estimator_dict.keys():\n",
    "        try:\n",
    "            strat_run_time = run_times_per_estimator.loc[run_times_per_estimator['strategy_name']==strat]\n",
    "            in_sec = np.float(strat_run_time['total_seconds'])\n",
    "        except:\n",
    "            in_sec = np.nan\n",
    "        estimator_dict[strat].append(in_sec)\n",
    "        \n",
    "#         estimator_dict[strat].append(in_sec)\n",
    "    #check whether we have data on all estimators that were passed as an argument\n",
    "#     run_times_all_estimators_exist = (set(run_times_estimator_names) == set(estimator_dict.keys()))\n",
    "#     if exact_match and not run_times_all_estimators_exist:\n",
    "#         continue\n",
    "#     #TODO come up with a more efficient solution to avoid loop\n",
    "#     for i in range(run_times_per_estimator.shape[0]):\n",
    "\n",
    "#         strategy_name = run_times_per_estimator.iloc[i]['strategy_name']\n",
    "#         total_seconds = run_times_per_estimator.iloc[i]['total_seconds']\n",
    "#         estimator_dict[strategy_name].append(total_seconds)\n",
    "# #the long notation is necessary to handle situations when there are unequal number of obeservations per estimator\n",
    "# training_time_per_dataset = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in estimator_dict.items() ]))\n",
    "# training_time_per_dataset = training_time_per_dataset.round(3)\n",
    "# avg_training_time = pd.DataFrame(training_time_per_dataset.mean(axis=0))\n",
    "# avg_training_time.columns = ['avg training time (in sec)']\n",
    "# avg_training_time = avg_training_time.sort_values('avg training time (in sec)',ascending=True).round(3)\n",
    "# return avg_training_time, training_time_per_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_times_per_estimator['strategy_name'] == 'dd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.float(strat_run_time['total_seconds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame.from_dict(estimator_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.mean(axis=0, numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_times_per_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = run_times_per_estimator.loc[run_times_per_estimator['strategy_name']=='SVC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.float(b['total_seconds'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
