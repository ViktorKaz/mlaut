from sklearn.metrics import accuracy_score, mean_squared_error
import collections
import numpy as np
import pandas as pd
class Losses(object):
    """
    Calculates prediction losses on test datasets achieved by the trained estimators. When the class is instantiated it creates a dictionary that stores the losses.

    Parameters
    ----------
    metric(string): loss metric that will be calculated.
    round_round_predictions(Boolean): Sets whether the predictions of the estimators should be rounded to the nearest integer. This is useful when calculating the accuracy scores of the outputs of estimators that produce regression results as opposed to classification results. The default behaviour is to round the predictions if the ``accuracy`` metric is used.   

    """

    def __init__(self, metric, round_predictions=None):

        self._losses = collections.defaultdict(list)
        self._metric = metric

        if round_predictions is None and metric is 'accuracy':
            self._round_predictions = True
        else:
            self._round_predictions = round_predictions

    def evaluate(self, predictions, true_labels):
        """
        Calculates the loss metrics on the test sets.

        :type predictions: 2d numpy array
        :param v: Predictions of trained estimators in the form [estimator_name, [predictions]]

        :type true_labels: numpy array
        :param true_labels: true labels of test dataset.
        """
        
        for prediction in predictions:
            estimator_name = prediction[0]
            estimator_predictions = prediction[1]
            if self._round_predictions is True:
                estimator_predictions = np.rint(estimator_predictions)
            
            
            if self._metric is 'accuracy':
                loss = accuracy_score(true_labels, estimator_predictions)
                self._losses[estimator_name].append( loss )
            elif self._metric is 'mean_squared_error':
                loss = mean_squared_error(true_labels, estimator_predictions)
                self._losses[estimator_name].append(loss)   
            else:
                raise ValueError(f'metric {self._metric} is not supported.')
            
    def evaluate_per_dataset(self, 
                            predictions, 
                            true_labels, 
                            dataset_name):

        """
        Calculates the error of an estimator per dataset.
        
        Parameters
        ----------
        predictions : 2d array-like in the form [estimator name, [estimator_predictions]].
        true_labels : 1d array-like
        
        """
        estimator_name = predictions[0]
        estimator_predictions = np.array(predictions[1])
        errors = (estimator_predictions - true_labels)**2
        n = len(errors)

        std_score = np.std(errors)/np.sqrt(n) 
        sum_score = np.sum(errors)
        avg_score = sum_score/n
        self._losses[dataset_name].append([estimator_name, avg_score, std_score])

    def get_losses(self):
        """
        When the Losses class is instantiated a dictionary that holds all losses is created and appended every time the evaluate() method is run. This method returns this dictionary with the losses.

        :rtype: dictionary
        """ 
        return self._losses

    def losses_to_dataframe(self, losses):
        """
        Reformats the output of the dictionary returned by the :func:`mleap.analyze_results.losses.Losses.get_losses` to a pandas DataFrame. This method can only be applied to reformat the output produced by :func:`mleap.analyze_results.Losses.evaluate_per_dataset`.

        Parameters
        ----------

        losses: dictionary returned by the :func:`mleap.analyze_results.losses.Losses.get_losses` generated by :func:`mleap.analyze_results.losses.Losses.evaluate_per_dataset`
        """

        df = pd.DataFrame(losses)
        #unpivot the data
        df = df.melt(var_name='dts', value_name='values')
        df['classifier'] = df.apply(lambda raw: raw.values[1][0], axis=1)
        df['score'] = df.apply(lambda raw: raw.values[1][1], axis=1)
        df['std'] = df.apply(lambda raw: raw.values[1][2], axis=1)
        df = df.drop('values', axis=1)
        #create multilevel index dataframe
        dts = df['dts'].unique()
        estimators_list = df['classifier'].unique()
        score = df['score'].values
        std = df['std'].values
        
        df = df.drop('dts', axis=1)
        df=df.drop('classifier', axis=1)
        
        df.index = pd.MultiIndex.from_product([dts, estimators_list])

        return df