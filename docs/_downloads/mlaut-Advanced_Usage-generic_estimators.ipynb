{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from download_delgado.delgado_datasets import DownloadAndConvertDelgadoDatasets\n",
    "from mlaut.data import Data\n",
    "from mlaut.estimators.estimators import instantiate_default_estimators\n",
    "from mlaut.experiments import Orchestrator\n",
    "from mlaut.analyze_results import AnalyseResults\n",
    "from download_delgado.delgado_datasets import DownloadAndConvertDelgadoDatasets\n",
    "from mlaut.analyze_results.scores import ScoreAccuracy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlaut.estimators.generic_estimator import Generic_Estimator\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlaut.estimators.nn_estimators import Deep_NN_Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLAUT is a modelling and workflow toolbox in python, written with the aim of simplifying large scale benchmarking of machine learning strategies, e.g., validation, evaluation and comparison with respect to predictive/task-specific performance or runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will demonstrate how the user can define from scratch his own estimator objects and use them for running experiments.\n",
    "\n",
    "the diagram below sketches the typical MLAUT workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/workflow.png?2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Download the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the puposes of this demonstration we will use datasets that can be downloaded from:\n",
    "https://persoal.citius.usc.es/manuel.fernandez.delgado/papers/jmlr/data.tar.gz\n",
    "\n",
    "The datasets were used in the study <cite data-cite=\"delgado2014\">Do we Need Hundreds of Classifiers to solve Real World Classification Problems? (Delgado, 2014)</cite> and were originally downloaded from the UCI Machine Learning Repository.\n",
    "\n",
    "The code below uses this data and creates two arrays. The first one contains the actual datasets and the scond one the metadata associated with each dataset. The metadata should be in the form of a dictionary and must contain at a minimum \n",
    "\n",
    "```python\n",
    "{'class_name': ..., #name of the column containing the labels\n",
    " 'dataset_name': ... #name of the dataset\n",
    "}\n",
    "```\n",
    "\n",
    "This step is not MLAUT specific but the data needs to be stored in a MLAUT compatible format so that the experiments can be run.\n",
    "\n",
    "In this example we assume that all datasets can fit into memory and be saved with one command in the database. If this is not the case the user break this process on a dataset by dataset basis or make it even more granular. This step is not MLAUT-specific so it is not discussed in detail in this demo.\n",
    "\n",
    "The code for downloading the collection of datasets is also not MLAUT-specific and will not be discussed in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Dataset Delgado_data/molec-biol-protein-second has a different number of arff files\n"
     ]
    }
   ],
   "source": [
    "delgado = DownloadAndConvertDelgadoDatasets()\n",
    "datasets, metadata = delgado.download_and_extract_datasets(verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`datasets`: array of pandas dataframe containing all datasets<br>\n",
    "`metadata`: array of dictionaries containing the metadata for each dataset. <br>\n",
    "\n",
    "The (`dataset`, `metadata`) tuples need to be ordered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Store the datasets in MLAUT format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to store the datasets in a format that can be used in MLAUT.\n",
    "\n",
    "Interaction with MLAUT's data structures is done though the `Data()` class that serves as interface for storing and loading data into HDF5 database which is used by MLAUT for data maniputlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data()\n",
    "input_io = data.open_hdf5('data/delgado.hdf5', mode='a')\n",
    "out_io = data.open_hdf5('data/classification.hdf5', mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to store the data in the HDF5 database. \n",
    "\n",
    "`save_loc_hdf5` indicates the HDF5 group in which the datasets will be saved.<br>\n",
    "`datasets` are the actual datasets in pandas format.<br>\n",
    "`dts_metadata` is the metadata array attached to each dataset.<br>\n",
    "`input_io` is the class object that interfaces the HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.pandas_to_db(save_loc_hdf5='delgado_datasets/', datasets=datasets, \n",
    "                  dts_metadata=metadata, input_io=input_io)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Split datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to split the data in test and training. \n",
    "\n",
    "Unless otherwise specified we use $\\dfrac{2}{3}$ of the data for training and $\\dfrac{1}{3}$ for testing. We do not change or move the original data in this process. Instead we store the train/test indices in a separate HDF5 database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dts_names_list, dts_names_list_full_path = data.list_datasets(hdf5_io=input_io, hdf5_group='delgado_datasets/')\n",
    "split_dts_list = data.split_datasets(hdf5_in=input_io, hdf5_out=out_io, dataset_paths=dts_names_list_full_path, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dts_names_list`: names of the datasets saved inside the HDF5 file <br>\n",
    "`dts_names_list_full_path`: full path to the datasets inside the HDF5 database <br>\n",
    "`split_dts_list`: path to the train/test indices of the split datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Define the estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this advanced use case example we will show how the user can create its own estimator objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop = {'estimator_family':['SVM'], \n",
    "            'tasks':['Classification'], \n",
    "            'name':'SVC'}\n",
    "\n",
    "hyperparametes = {\n",
    "    \n",
    "                    'C': np.linspace(2**(-5), 2**(15), 13),\n",
    "                    'gamma': np.linspace(2**(-15), 2**3, 13)\n",
    "                        \n",
    "}\n",
    "est = GridSearchCV(\n",
    "    SVC(), \n",
    "    hyperparametes, \n",
    "    verbose = True ,\n",
    "    n_jobs=-1)\n",
    "\n",
    "SVC = Generic_Estimator(\n",
    "    properties_dict = prop, \n",
    "            \n",
    "            estimator=est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop = {'estimator_family':['ENSEMBLE_METHODS'], \n",
    "            'tasks':['CLASSIFICATION'], \n",
    "            'name':'RandomForestClassifier'}\n",
    "hyperparameters = {\n",
    "                'n_estimators': [10, 50, 100],\n",
    "                'max_features': ['auto', 'sqrt','log2', None],\n",
    "                'max_depth': [5, 15, None]\n",
    "                }\n",
    "estimator = GridSearchCV(RandomForestClassifier(), \n",
    "                    hyperparameters, \n",
    "                    verbose =  True,\n",
    "                    n_jobs= -1)\n",
    "RF = Generic_Estimator(\n",
    "    properties_dict = prop, \n",
    "            estimator=est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop = {'estimator_family':['NEURAL_NETWORKS'], \n",
    "            'tasks':['CLASSIFICATION'], \n",
    "            'name':'NN-3layer'}\n",
    "\n",
    "hyperparameters = {'epochs': [50,100], \n",
    "                    'batch_size': 0,  \n",
    "                    'learning_rate':0.001,\n",
    "                    'loss': 'mean_squared_error',\n",
    "                    'optimizer': 'Adam',\n",
    "                    'metrics' : ['accuracy']}\n",
    "def keras_model(num_classes, input_dim):\n",
    "    nn_deep_model = OverwrittenSequentialClassifier()\n",
    "    nn_deep_model.add(Dense(288, input_dim=input_dim, activation='relu'))\n",
    "    nn_deep_model.add(Dense(144, activation='relu'))\n",
    "    nn_deep_model.add(Dropout(0.5))\n",
    "    nn_deep_model.add(Dense(12, activation='relu'))\n",
    "    nn_deep_model.add(Dense(num_classes, activation='softmax'))\n",
    "    return nn_deep_model\n",
    "deep_nn = Deep_NN_Classifier(hyperparameters=hyperparameters,\n",
    "                            keras_model=keras_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note the different approach to defining custom estimators.\n",
    "\n",
    "The user can either start with a blank canvas and construct an estimator object from a `Generic_Estimator` class or use an MLAUT estimator (`Deep_NN_Classifier` in this example) and customize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [SVC, RF, deep_nn]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Run the experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this step we need to select the estimators that we want to use in the study. In this example we enumerated the estimators by name. However, MLAUT also supports a search by task or estimator familily.\n",
    "\n",
    "The user also needs to instantiate the test orchestrator object by providing reference to the input and output database files and the location of the datasets inside the HDF5 database.\n",
    "\n",
    "The final step is to run the experiments by invoking the `run()` method.\n",
    "\n",
    "This step could take a substantial amount of time depending on the number and size of datasets and the number of estimators that we wish to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "orchest = Orchestrator(hdf5_input_io=input_io, hdf5_output_io=out_io, dts_names=dts_names_list,\n",
    "                 original_datasets_group_h5_path='delgado_datasets/')\n",
    "orchest.run(modelling_strategies=estimators, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Make predictions on the test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the estimators are trained the user needs to use them in order to make predictions on the test sets which will be used subsequently for performing statistical tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orchest.predict_all(trained_models_dir='data/trained_models', estimators=estimators, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Analyze the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step in the pipeline is to analyze the results of the experiments.\n",
    "\n",
    "The `AnalyseResults` class takes as inputs the two database files and the loss metric that will be used to compute the prediction errors.\n",
    "\n",
    "The `prediction_errors()` method retuns two sets of results: `errors_per_estimator` dictionary which is used subsequently in further statistical tests and `errors_per_dataset_per_estimator_df` which is a dataframe with the loss of each estimator on each dataset which can be examined directly by the user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze = AnalyseResults(hdf5_output_io=out_io, \n",
    "                         hdf5_input_io=input_io,\n",
    "                         input_h5_original_datasets_group='delgado_datasets/', \n",
    "                         output_h5_predictions_group='experiments/predictions/')\n",
    "score_accuracy = ScoreAccuracy()\n",
    "\n",
    "\n",
    "(errors_per_estimator, \n",
    " errors_per_dataset_per_estimator, \n",
    " errors_per_dataset_per_estimator_df) = analyze.prediction_errors(score_accuracy, estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we show the results of the various statistical tests that are supported by MLAUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test, t_test_df = analyze.t_test(errors_per_estimator)\n",
    "t_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sign test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_test, sign_test_df = analyze.sign_test(errors_per_estimator)\n",
    "sign_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-test with bonferroni correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_bonferroni_df = analyze.t_test_with_bonferroni_correction(errors_per_estimator)\n",
    "t_test_bonferroni_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wilcoxon test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "wilcoxon_test, wilcoxon_test_df = analyze.wilcoxon_test(errors_per_estimator)\n",
    "wilcoxon_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Friedman test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friedman_test, friedman_test_df = analyze.friedman_test(errors_per_estimator)\n",
    "friedman_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nemenyi test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nemeniy_test = analyze.nemenyi(errors_per_estimator)\n",
    "nemeniy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nemeniy_test = analyze.nemenyi(errors_per_estimator)\n",
    "nemeniy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 5000)\n",
    "errors_per_dataset_per_estimator_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
