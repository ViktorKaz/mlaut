Tutorial
========

Basic Usage
-----------

The code below will help you get started with mlaut.

The following Jupyter Notebook contains the code used below.

:download:`mlaut_Basic_Usage.ipynb <../examples/mlaut_Basic_Usage.ipynb>`.


**1. Establish hooks to the database that stores the data for the experiments.**

The enclosed code can be used as follows:  

.. literalinclude:: ../examples/basic.py
    :language: python
    :lines: 9-10

:`input_io`: hook to the input HDF5 database file
:`out_io`:   hook to the output HDF5 database file

**2. Split the datasets in train and test.**

After the hooks are created we can proceed to splitting the data in test and training. 

Unless otherwise specified we use :math:`\dfrac{2}{3}` of the data for training and :math:`\dfrac{1}{3}` for testing. We do not change or move the original data in this process. Instead we store the train/test indices in a separate HDF5 database.

.. literalinclude:: ../examples/basic.py
    :language: python
    :lines: 14-15

:`dts_names_list`: names of the datasets saved inside the HDF5 file
:`dts_names_list_full_path`: full path to the datasets inside the HDF5 database
:`split_dts_list`: path to the train/test indices of the split datasets

**3. Define the estimators.**

For the puposes of the basic demo we show how the standard set of estimaots that come with MLAUT can be used for running the experiments. 

In the code example below we enumerate by name the estimators that we wish to use in the study. This will provide instances of MLAUT estimators with the built in defaults.

For more advanced used cases pleaes refer to :download:`mlaut-Advanced_Usage-generic_estimators.ipynb <../examples/mlaut-Advanced_Usage-generic_estimators.ipynb>` and :download:`mlaut-Advanced_Usage-hyperparameters.ipynb <../examples/mlaut-Advanced_Usage-hyperparameters.ipynb>` which show how the user can easily change the hyper paramemeter defaults or define a completely new estimator object.

.. literalinclude:: ../examples/basic.py
    :language: python
    :lines: 18-19

:`estimators`: array of ``mlaut`` estimators

**4. Run the experiments.**

The final step is to run the experiments by invoking the ``run()`` method.

This step could take a substantial amount of time depending on the number and size of datasets and the number of estimators that we wish to train.

All trained estimators are saved on the HDD.


.. literalinclude:: ../examples/basic.py
    :language: python
    :lines: 22-24

One of the key feautres of the package is to allow for the experiment to resume in case of a crash or interruption. If this happens, the user would simply need to re-run the code above. Unless the `override_saved_models=True` flag was set the orchestrator will skip all estimators that were trained sucessfully. This would allow the user to continue from the point where the experiments were stopped.

**5. Make predictions on the test sets.**

After the estimators are trained the user needs to use them in order to make predictions on the test sets which will be used subsequently for performing statistical tests.

The predictions of the estimators are saved in the input HDF5 database file a hook to which was created earlier.

Unless the `override=False` flag was set MLAUT will not override predictions that were previously stored in the database.

.. literalinclude:: ../examples/basic.py
    :language: python
    :lines: 27

**6. Analyze results.**

The last step in the pipeline is to analyze the results of the experiments.

The `AnalyseResults` class takes as inputs the two database files and the loss metric that will be used to compute the prediction errors.

.. literalinclude:: ../examples/basic.py
    :language: python
    :lines: 30-39

The `prediction_errors()` method retuns two sets of results: `errors_per_estimator` dictionary which is used subsequently in further statistical tests and `errors_per_dataset_per_estimator_df` which is a dataframe with the loss of each estimator on each dataset which can be examined directly by the user. 

The `errors_per_estimator` result can be used as input to the following functions to perform the different statistical tests supported by ``mlaut``.

* :meth:`mlaut.analyze_results.analyze_results.calculate_average_std`
* :meth:`mlaut.analyze_results.analyze_results.cohens_d`
* :meth:`mlaut.analyze_results.analyze_results.t_test`
* :meth:`mlaut.analyze_results.analyze_results.sign_test`
* :meth:`mlaut.analyze_results.analyze_results.t_test_with_bonferroni_correction`
* :meth:`mlaut.analyze_results.analyze_results.wilcoxon_test`
* :meth:`mlaut.analyze_results.analyze_results.ranksum_test`
* :meth:`mlaut.analyze_results.analyze_results.friedman_test`
* :meth:`mlaut.analyze_results.analyze_results.nemenyi`

Please refer to :ref:`analyze_results` for additional information about their use.

MLAUT Large Scale ML Benchmarking Study
---------------------------------

We used ``mlaut`` to perform a large scale benchmarking study for a number of machine learning models. In doing so we recreated and expanded a previous paper (:cite:`fernandez-delgado_we_2014`).

The code for our study can be downloaded from the following link :download:`mlaut_study <../examples/mlaut_study.zip>` and can also be used as an advanced tutorial and reference for using ``mlaut``.


.. bibliography:: references.bib
