{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from download_delgado.delgado_datasets import DownloadAndConvertDelgadoDatasets\n",
    "from mleap.data import Data\n",
    "from mleap.data.estimators import instantiate_default_estimators\n",
    "from mleap.experiments import TestOrchestrator\n",
    "from mleap.analyze_results import AnalyseResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Dataset Delgado_data\\molec-biol-protein-second has a different number of arff files\n"
     ]
    }
   ],
   "source": [
    "#load datasets as pandas DataFrame\n",
    "#each dataset needs to have metadata attached to it containing the following keys:\n",
    "# class_name: (string) name of column containing the column name in which the labels are stored\n",
    "# dataset_name: (string) name of the dataset\n",
    "delgado = DownloadAndConvertDelgadoDatasets()\n",
    "datasets, metadata = delgado.download_and_extract_datasets(verbose = False) #think about storing in hdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Save the datasets in HDF5 database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files io object needs to be used in test orch\n",
    "# returned object needs to be integrated in code\n",
    "data = Data()\n",
    "data.pandas_to_db(save_loc_hdf5='delgado_datasets/', datasets=datasets, \n",
    "                  dts_metadata=metadata, save_loc_hdd='data/delgado.hdf5') #return files io object\n",
    "input_io = data.open_hdf5('data/delgado.hdf5', mode='r')\n",
    "out_io = data.open_hdf5('data/experiments.hdf5', mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Instantiate models and Test Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "instantiated_models = instantiate_default_estimators(estimators=['all'])\n",
    "\n",
    "test_o = TestOrchestrator(hdf5_input_io=input_io, hdf5_output_io=out_io)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Split datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dts_names_list, dts_names_list_full_path = data.list_datasets(hdf5_io=input_io, hdf5_group='delgado_datasets/')\n",
    "split_dts_list = data.split_datasets(hdf5_in=input_io, hdf5_out=out_io, dataset_paths=dts_names_list_full_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Run the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Training models on dataset: abalone. Total datasets processed: 0/3 ***\n",
      "Epoch 1/10\n",
      "2798/2798 [==============================] - 0s - loss: 0.1677 - acc: 0.5933     \n",
      "Epoch 2/10\n",
      "2798/2798 [==============================] - 0s - loss: 0.1546 - acc: 0.6387     \n",
      "Epoch 3/10\n",
      "2798/2798 [==============================] - 0s - loss: 0.1500 - acc: 0.6447     \n",
      "Epoch 4/10\n",
      "2798/2798 [==============================] - 0s - loss: 0.1493 - acc: 0.6487     \n",
      "Epoch 5/10\n",
      "2798/2798 [==============================] - 0s - loss: 0.1467 - acc: 0.6583     \n",
      "Epoch 6/10\n",
      "2798/2798 [==============================] - 0s - loss: 0.1457 - acc: 0.6580     \n",
      "Epoch 7/10\n",
      "2798/2798 [==============================] - 0s - loss: 0.1472 - acc: 0.6530     \n",
      "Epoch 8/10\n",
      "2798/2798 [==============================] - 0s - loss: 0.1452 - acc: 0.6640     \n",
      "Epoch 9/10\n",
      "2798/2798 [==============================] - 0s - loss: 0.1444 - acc: 0.6605     \n",
      "Epoch 10/10\n",
      "2798/2798 [==============================] - 0s - loss: 0.1432 - acc: 0.6605     \n",
      "1152/1379 [========================>.....] - ETA: 0s*** Training models on dataset: acute_inflammation. Total datasets processed: 1/3 ***\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 0s - loss: 0.2428 - acc: 0.5625     \n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 0s - loss: 0.1735 - acc: 0.9625     \n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 0s - loss: 0.1304 - acc: 0.9625     \n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 0s - loss: 0.0801 - acc: 1.0000     \n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 0s - loss: 0.0563 - acc: 0.9875     \n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 0s - loss: 0.0416 - acc: 0.9875     \n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 0s - loss: 0.0288 - acc: 0.9875     \n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 0s - loss: 0.0152 - acc: 1.0000     \n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 0s - loss: 0.0129 - acc: 1.0000     \n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 0s - loss: 0.0080 - acc: 1.0000     \n",
      "32/40 [=======================>......] - ETA: 0s*** Training models on dataset: acute_nephritis. Total datasets processed: 2/3 ***\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 0s - loss: 0.1989 - acc: 0.8375     \n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 0s - loss: 0.1222 - acc: 0.9875     \n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 0s - loss: 0.0667 - acc: 0.9875     \n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 0s - loss: 0.0365 - acc: 1.0000     \n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 0s - loss: 0.0178 - acc: 1.0000     \n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 0s - loss: 0.0102 - acc: 1.0000     \n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 0s - loss: 0.0064 - acc: 1.0000     \n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 0s - loss: 0.0025 - acc: 1.0000     \n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 0s - loss: 0.0013 - acc: 1.0000     \n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 0s - loss: 7.8337e-04 - acc: 1.0000     \n",
      "32/40 [=======================>......] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "test_o.run(input_io_datasets_loc=dts_names_list_full_path[0:3], output_io_split_idx_loc=split_dts_list[0:3], modelling_strategies=instantiated_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Analyze the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******t-test******\n",
      "                                                 pair  t_statistic  p_value\n",
      "0            BernoulliNaiveBayes - GaussianNaiveBayes         2.92     0.57\n",
      "1            BernoulliNaiveBayes - LogisticRegression         2.92     0.24\n",
      "2   BernoulliNaiveBayes - NeuralNetworkDeepClassifier         2.92     0.15\n",
      "3        BernoulliNaiveBayes - RandomForestClassifier         2.92     0.15\n",
      "4                           BernoulliNaiveBayes - SVC         2.92     0.16\n",
      "5             GaussianNaiveBayes - LogisticRegression         2.92     0.07\n",
      "6    GaussianNaiveBayes - NeuralNetworkDeepClassifier         2.92     0.09\n",
      "7         GaussianNaiveBayes - RandomForestClassifier         2.92     0.09\n",
      "8                            GaussianNaiveBayes - SVC         2.92     0.09\n",
      "9    LogisticRegression - NeuralNetworkDeepClassifier         2.92     0.16\n",
      "10        LogisticRegression - RandomForestClassifier         2.92     0.18\n",
      "11                           LogisticRegression - SVC         2.92     0.14\n",
      "12  NeuralNetworkDeepClassifier - RandomForestClas...         2.92     0.35\n",
      "13                  NeuralNetworkDeepClassifier - SVC         2.92     0.35\n",
      "14                       RandomForestClassifier - SVC         2.92     0.35\n"
     ]
    }
   ],
   "source": [
    "analyze = AnalyseResults(hdf5_output_io=out_io, hdf5_input_io=input_io)\n",
    "errors = analyze.calculate_loss_all_datasets(input_h5_original_datasets_group='delgado_datasets/', \n",
    "                                    output_h5_predictions_group='experiments/predictions/', \n",
    "                                    metric='mean_squared_error')\n",
    "\n",
    "t_test, t_test_df = analyze.perform_t_test(errors)\n",
    "print('******t-test******')\n",
    "print(t_test_df)\n",
    "\n",
    "# sign_test, sign_test_df = analyze.perform_sign_test()\n",
    "# print('******sign test******')\n",
    "# print(sign_test_df)\n",
    "\n",
    "# t_test_bonferroni, t_test_bonferroni_df = analyze.perform_t_test_with_bonferroni_correction()\n",
    "# print('******t-test bonferroni correction******')\n",
    "# print(t_test_bonferroni_df)\n",
    "# wilcoxon_test, wilcoxon_test_df = analyze.perform_wilcoxon()\n",
    "# print('******Wilcoxon test******')\n",
    "# print(wilcoxon_test_df)\n",
    "\n",
    "# friedman_test, friedman_test_df = analyze.perform_friedman_test()\n",
    "# print('******Friedman test******')\n",
    "# print(friedman_test_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
